# ğŸ“© SMS Spam Classifier

## Overview
This project implements an SMS spam classifier using the **NaÃ¯ve Bayes algorithm** in R. The model is trained on a dataset of spam and ham messages and optimized for accuracy.

## ğŸ“‚ Project Structure
```
ğŸ“¦ SMS-Spam-Classifier
â”œâ”€â”€ ğŸ“„ README.md  
â”œâ”€â”€ ğŸ“‚ data  
â”‚   â”œâ”€â”€ spam.csv  
â”œâ”€â”€ ğŸ“‚ notebooks  
â”‚   â”œâ”€â”€ SPAM-Filter.ipynb  
â””â”€â”€ ğŸ“‚ visualization  
    â”œâ”€â”€ word cloud. Spam Vs Ham.png  
```

## ğŸ“Š Data Cleaning and Preprocessing
1. **Import Data** 
Kaggle link: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset

2. **Text Processing** 
- converting all the texts to lowercase
- removing Numbers
- removing stopwords
- removing Punctuation
- removing white spaces

## ğŸ¨ Visualization
- **Word Cloud** â˜ï¸
```r
install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus, min.freq = 40, random.order = FALSE)
```

## ğŸ¤– Model Training
**NaÃ¯ve Bayes Classifier** 
```r
install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw$type)
```

## ğŸ“ˆ Model Evaluation
Looking at the table, we can see that only **0.41 percent** were incorrectly classified as spam, while 14.44 percent were incorrectly classified as ham. 

Considering the little effort we put in, this level of performance seems quite impressive. This case study exemplifies the reason why **naive Bayes is the standard for text classification**; directly out of the box, it performs surprisingly well

## ğŸš€ Optimizing Performance
When training our model, we didn't set the Laplace parameter when running the naive Bayes(), thus, we will include it right now.

ğŸ”¹ **False positives reduced from 5 to 2!**

ğŸ”¹ **False negatives reduced from 26 to 20!**

## ğŸ¯ Conclusion
This project demonstrates the effectiveness of the **NaÃ¯ve Bayes classifier** in text classification, particularly for **SMS spam detection**. ğŸš€

### ğŸŒŸ Author
ğŸ“Œ **Oumayma Abayed** | Data Science Enthusiast


